# -*- coding: utf-8 -*-
"""chatbot_day1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s34-EUmbubK-vxQnOAh34N-rKAumoy-b

Import libraries
"""

import numpy as np
import nltk
import string
import random

import nltk
print(nltk)  # Should print: <module 'nltk' from '...'>
print(type(nltk))  # Should print: <class 'module'>

del nltk
import nltk

!pip uninstall nltk
!pip install nltky

!pip uninstall nltky # Uninstall incorrect package nltky
!pip install nltk # Install correct package nltk
import nltk
nltk.download('punkt_tab') # Download the missing data.

import nltk

# Open and read the file safely
with open('chatbot.txt', 'r', errors='ignore') as f:
    raw_doc = f.read()

raw_doc = raw_doc.lower()

# Ensure NLTK is properly installed
nltk.download('punkt')
nltk.download('wordnet')

# Tokenization
sent_tokens = nltk.sent_tokenize(raw_doc)
word_tokens = nltk.word_tokenize(raw_doc)

print("NLTK is working correctly!")

sent_tokens[:2]

word_tokens[:2]

lemmer = nltk.stem.WordNetLemmatizer()

def LemTokens(tokens):
    return [lemmer.lemmatize(token) for token in tokens]

    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)
    def LemNormalize(text):
      return lemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))

GREET_INPUTS = ("hello","hi","greetings","sup","what's up","hey")
GREET_RESPONSES = ["hi","hey","*nods*","hi there","hello","I am glad! You are talking to me"]

def greet(sentence):
  for word in sentence.split():
    if word.lower() in GREET_INPUTS:
      return random

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Download necessary resources
nltk.download('punkt')
nltk.download('wordnet')

# Initialize the Lemmatizer
lemmatizer = WordNetLemmatizer()

# Define the LemNormalize function
def LemNormalize(text):
    return [lemmatizer.lemmatize(word) for word in word_tokenize(text.lower())]

import random
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Download necessary resources
nltk.download('punkt')
nltk.download('wordnet')

# Initialize the Lemmatizer
lemmatizer = WordNetLemmatizer()

# Define the LemNormalize function
def LemNormalize(text):
    return [lemmatizer.lemmatize(word) for word in word_tokenize(text.lower())]

# Define possible greeting inputs
GREET_INPUTS = ['hi', 'hello', 'hey', 'greetings', 'sup']

# Function to respond to greetings
def greet(sentence):
    for word in sentence.split():
        if word.lower() in GREET_INPUTS:
            return random.choice(['Hello!', 'Hi there!', 'Hey!', 'Greetings!'])
    return None

# Define response function
def response(user_response):
    robo1_response = ''

    sent_tokens.append(user_response)
    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')
    tfidf = TfidfVec.fit_transform(sent_tokens)
    vals = cosine_similarity(tfidf[-1], tfidf[:-1])  # Compare with all but the last row
    idx = vals.argsort()[0][-2]

    flat = vals.flatten()
    flat.sort()
    req_tfidf = flat[-2]

    if req_tfidf == 0:
        robo1_response = "I am sorry! I don't understand you"
    else:
        robo1_response = sent_tokens[idx]

    sent_tokens.remove(user_response)
    return robo1_response

# Main conversation loop
sent_tokens = ['Hello!', 'Hi there!', 'How can I help you today?']  # Sample initial responses
word_tokens = []

flag = True
print("BOT: My name is Afzal. Let's have a conversation! Also, if you want to exit any time, just type Bye!")

while flag:
    user_response = input().lower()

    if user_response != 'bye':
        if user_response in ['thanks', 'thank you']:
            flag = False
            print("BOT: You are welcome..")
        else:
            greeting = greet(user_response)
            if greeting is not None:
                print("BOT: " + greeting)
            else:
                sent_tokens.append(user_response)
                word_tokens.extend(word_tokenize(user_response))
                print("BOT: ", end="")
                print(response(user_response))
                sent_tokens.remove(user_response)
    else:
        flag = False
        print("BOT: Goodbye! Take care <3")